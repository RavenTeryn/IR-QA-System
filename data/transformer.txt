在深度学习中，transformer（直译为“变换器”）是一种基于多头注意力机制的人工神经网络架构，其中文本被转换为称为词元（token）的数值表示，每个词元通过从词嵌入表中查找转换为一个向量。[1]在每一层，每个词元都通过并行的多头注意力机制在上下文窗口的范围内与其他（未屏蔽的）词元进行上下文关联，从而放大关键词元的信号并减弱不太重要的词元，从而按输入数据各部分重要性的不同而分配不同的权重。采用该架构的模型主要用于自然语言处理（NLP）与计算机视觉（CV）领域。

和循环神经网络（RNN）一样，transformer旨在处理自然语言等顺序输入数据，可应用于翻译、文本摘要等任务。但transformer的优势在于它没有循环单元，能够一次性处理所有输入数据。注意力机制可以为输入序列中的任意位置提供上下文。如果输入数据是自然语言，则transformer不必像RNN一样一次只处理一个单词，这种架构允许更多的并行计算，并以此减少训练时间。[1]之后的变体被广泛用于在大型（语言）数据集上训练大型语言模型（LLM）。[2]

现代版transformer于2017年由谷歌大脑的一个团队在论文《Attention Is All You Need》中提出。[1] 其前身最初是为了改进机器翻译的现有架构而开发的，[3][4] 但此后应用范围不断扩大。它们被用于大规模自然语言处理、计算机视觉（视觉transformer）[5]、强化学习[6][7]、音频处理[8]、多模态学习、机器人[9]，甚至可以下国际象棋[10]，逐步取代长短期记忆（LSTM）等RNN模型成为了NLP问题的首选模型[11]。Transformer模型也催生了预训练系统（迁移学习）的发展，并行化优势允许其在更大的数据集上进行训练，例如GPT 和BERT[12]。这些系统使用了维基百科、Common Crawl等大型语料库进行训练，并可以针对特定任务进行微调。[13][14]

跟2017年的原始模型相比，一些现代模型（如RoBERTa）在词元化阶段可以改用更高效的WordPiece算法；位置编码则可能使用更新的旋转位置编码(RoPE)方案[15]。