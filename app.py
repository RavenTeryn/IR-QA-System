import streamlit as st
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# --- 1. é¡µé¢è®¾ç½® (å¯¹åº”åŠŸèƒ½è¦æ±‚: User Interface) ---
st.set_page_config(page_title="æˆ‘çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ", layout="wide")
st.title("ğŸ¤– ç»´åŸºç™¾ç§‘æ™ºèƒ½é—®ç­”ç³»ç»Ÿ (QA Bot)")
st.write("æœ¬ç³»ç»ŸåŸºäº RAG æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸Šä¼ çš„çŸ¥è¯†åº“å›ç­”é—®é¢˜ã€‚")

# --- 2. åŠ è½½ä¸å¤„ç†æ•°æ®çš„å‡½æ•° ---
@st.cache_resource  # è¿™ä¸ªè£…é¥°å™¨è®©ç³»ç»Ÿä¸ç”¨æ¯æ¬¡åˆ·æ–°éƒ½é‡æ–°åŠ è½½æ¨¡å‹ï¼Œé€Ÿåº¦æ›´å¿«
def initialize_system():
    # A. æ£€æŸ¥ data æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists("data"):
        os.makedirs("data")
        st.warning("âš ï¸ 'data' æ–‡ä»¶å¤¹ä¸ºç©ºï¼è¯·æ”¾å…¥ .txt æ–‡ä»¶ååˆ·æ–°é¡µé¢ã€‚")
        # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ–‡ä»¶é˜²æ­¢æŠ¥é”™
        with open("data/sample.txt", "w", encoding='utf-8') as f:
            f.write("æ•…å®«ä½äºåŒ—äº¬ä¸­å¿ƒï¼Œæ˜¯æ˜æ¸…ä¸¤ä»£çš„çš‡å®«ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ã€‚")
    
    # B. åŠ è½½æ¨¡å‹ (å…³é”®ç‚¹ï¼šæ¢æˆä¸­æ–‡æ¨¡å‹ BAAI/bge-small-zh-v1.5)
    # ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´
    st.info("æ­£åœ¨åŠ è½½ä¸­æ–‡åµŒå…¥æ¨¡å‹ (BAAI/bge-small-zh-v1.5)...")
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5")
    
    # C. è¯»å– data æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰ txt æ–‡ä»¶
    loader = DirectoryLoader('data/', glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
    documents = loader.load()
    
    if not documents:
        return None, None

    # D. åˆ‡åˆ†æ–‡æ¡£ (Text Splitting)
    # æŠŠé•¿æ–‡ç« åˆ‡æˆ 200 å­—ä¸€æ®µï¼Œæ–¹ä¾¿æ£€ç´¢å®šä½
# æ”¹è¿›ç‰ˆï¼šåŠ å…¥ä¸­æ–‡æ ‡ç‚¹ç¬¦å·æ”¯æŒï¼Œå¹¶ç¨å¾®åŠ å¤§åˆ†å—å¤§å°
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,        # æŠŠå—å¤§å°ä»200å¢åŠ åˆ°300ï¼Œä¿è¯èƒ½åŒ…å«æ›´å¤šä¸Šä¸‹æ–‡
        chunk_overlap=50,      # é‡å éƒ¨åˆ†ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡ä¸¢å¤±
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", "ã€", ""] # ä¼˜å…ˆçº§ï¼šå…ˆæŒ‰æ®µè½åˆ‡ï¼Œå†æŒ‰å¥å·åˆ‡
    )
    splits = text_splitter.split_documents(documents)
    
    # E. å»ºç«‹å‘é‡ç´¢å¼• (Retrieval Module)
    vector_db = FAISS.from_documents(splits, embeddings)
    
    return vector_db, documents

# --- 3. åˆå§‹åŒ–ç³»ç»Ÿ ---
vector_db, raw_docs = initialize_system()

# ä¾§è¾¹æ æ˜¾ç¤ºä¿¡æ¯
with st.sidebar:
    st.header("ğŸ“š çŸ¥è¯†åº“çŠ¶æ€")
    if raw_docs:
        st.success(f"å·²åŠ è½½ {len(raw_docs)} ç¯‡æ–‡ç« ")
        st.write("æ–‡ä»¶åˆ—è¡¨:")
        for doc in raw_docs:
            st.code(doc.metadata['source'].split('/')[-1]) # åªæ˜¾ç¤ºæ–‡ä»¶å
    else:
        st.error("æœªæ‰¾åˆ°æ–‡æ¡£ï¼Œè¯·åœ¨ data æ–‡ä»¶å¤¹ä¸­æ·»åŠ  txt æ–‡ä»¶ã€‚")

# --- 4. é—®ç­”äº¤äº’åŒºåŸŸ ---
# è¾“å…¥æ¡† (Input Query)
query = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š", placeholder="ä¾‹å¦‚ï¼šæ•…å®«æ˜¯å“ªä¸ªæœä»£å»ºç«‹çš„ï¼Ÿ")

if query and vector_db:
    # æ£€ç´¢é€»è¾‘ (Retrieval)
    # k=3 è¡¨ç¤ºæ‰¾æœ€ç›¸ä¼¼çš„ 3 ä¸ªæ®µè½
    results = vector_db.similarity_search(query, k=3)
    
    st.markdown("### ğŸ” æ‰¾åˆ°çš„ç­”æ¡ˆæ®µè½ï¼š")
    
    # å±•ç¤ºç»“æœ
    for i, doc in enumerate(results):
        with st.expander(f"å‚è€ƒæ¥æº {i+1} (ç‚¹å‡»å±•å¼€/æ”¶èµ·)"):
            st.markdown(f"**å†…å®¹:** {doc.page_content}")
            st.caption(f"æ¥æºæ–‡ä»¶: {doc.metadata['source']}")
            
    # è¿™é‡Œå…¶å®å®Œæˆäº† Retrieve (æ£€ç´¢)ï¼Œä½ å¯ä»¥æŠŠæœ€ä¸Šé¢çš„ç»“æœå½“ä½œå³æ—¶ç­”æ¡ˆ
    st.success(f"æœ€ä½³ç­”æ¡ˆå¯èƒ½æ˜¯ï¼š\n\n{results[0].page_content}")

elif not vector_db:
    st.write("è¯·å…ˆå‡†å¤‡æ•°æ®ã€‚")